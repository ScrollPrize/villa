{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24f2370d-fa2c-4f6b-b384-3d53b9fb3119",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "torch.backends.cuda.enable_flash_sdp(True)\n",
    "\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Rearrange\n",
    "from dataclasses import dataclass\n",
    "from typing import Tuple\n",
    "\n",
    "# helpers\n",
    "\n",
    "def pair(t):\n",
    "    return t if isinstance(t, tuple) else (t, t)\n",
    "\n",
    "def triple(t):\n",
    "    return t if isinstance(t, tuple) else (t, t, t)\n",
    "\n",
    "# Configuration classes\n",
    "\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    \"\"\"Configuration for 3D ViT model parameters\"\"\"\n",
    "    image_size: int = 64\n",
    "    image_patch_size: int = 16\n",
    "    frames: int = 16\n",
    "    frame_patch_size: int = 4\n",
    "    num_classes: int = 2\n",
    "    dim: int = 384\n",
    "    depth: int = 6\n",
    "    heads: int = 6\n",
    "    mlp_dim: int = 1024\n",
    "    channels: int = 1\n",
    "    dim_head: int = 64\n",
    "    dropout: float = 0.1\n",
    "    emb_dropout: float = 0.1\n",
    "    flash_attn_type: str = 'pytorch'\n",
    "    \n",
    "    @property\n",
    "    def num_patches_h(self):\n",
    "        return self.image_size // self.image_patch_size\n",
    "    \n",
    "    @property\n",
    "    def num_patches_w(self):\n",
    "        return self.image_size // self.image_patch_size\n",
    "    \n",
    "    @property\n",
    "    def num_patches_f(self):\n",
    "        return self.frames // self.frame_patch_size\n",
    "    \n",
    "    @property\n",
    "    def total_patches(self):\n",
    "        return self.num_patches_h * self.num_patches_w * self.num_patches_f\n",
    "    \n",
    "    @property\n",
    "    def expected_output_shape(self):\n",
    "        \"\"\"Expected output shape for given batch size\"\"\"\n",
    "        return lambda batch_size: (batch_size, self.num_classes, self.num_patches_f, self.num_patches_h, self.num_patches_w)\n",
    "\n",
    "@dataclass\n",
    "class TestConfig:\n",
    "    \"\"\"Configuration for test data and benchmarking\"\"\"\n",
    "    batch_size: int = 2\n",
    "    benchmark_iterations: int = 20\n",
    "    warmup_iterations: int = 5\n",
    "    benchmark_seq_length: int = 1024\n",
    "    # Training test parameters\n",
    "    train_steps: int = 5\n",
    "    learning_rate: float = 1e-4\n",
    "    use_mixed_precision: bool = True\n",
    "    \n",
    "    def get_test_input_shape(self, config: ModelConfig):\n",
    "        \"\"\"Get test input shape based on model config\"\"\"\n",
    "        return (self.batch_size, config.channels, config.frames, config.image_size, config.image_size)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class FlashAttention(nn.Module):\n",
    "    def __init__(self, dim, heads = 8, dim_head = 64, dropout = 0., flash_attn_type='pytorch'):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head * heads\n",
    "        project_out = not (heads == 1 and dim_head == dim)\n",
    "\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "        self.flash_attn_type = flash_attn_type\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        ) if project_out else nn.Identity()\n",
    "\n",
    "        # Try to import flash_attn if using that backend\n",
    "        if flash_attn_type == 'flash_attn':\n",
    "            try:\n",
    "                from flash_attn import flash_attn_func\n",
    "                self.flash_attn_func = flash_attn_func\n",
    "                print(\"Using flash_attn package for attention\")\n",
    "            except ImportError:\n",
    "                print(\"flash_attn package not found, falling back to PyTorch SDPA\")\n",
    "                self.flash_attn_type = 'pytorch'\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm(x)\n",
    "        qkv = self.to_qkv(x).chunk(3, dim = -1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = self.heads), qkv)\n",
    "\n",
    "        # Determine dropout probability based on training mode\n",
    "        dropout_p = self.dropout if self.training else 0.0\n",
    "\n",
    "        if self.flash_attn_type == 'flash_attn' and hasattr(self, 'flash_attn_func'):\n",
    "            # Use dedicated flash_attn package\n",
    "            # Rearrange for flash_attn: (batch, seqlen, nheads, headdim)\n",
    "            q = rearrange(q, 'b h n d -> b n h d')\n",
    "            k = rearrange(k, 'b h n d -> b n h d')\n",
    "            v = rearrange(v, 'b h n d -> b n h d')\n",
    "            \n",
    "            out = self.flash_attn_func(\n",
    "                q, k, v,\n",
    "                dropout_p=dropout_p,\n",
    "                softmax_scale=self.scale,\n",
    "                causal=False\n",
    "            )\n",
    "            \n",
    "            # Rearrange back: (batch, seqlen, nheads, headdim) -> (batch, seqlen, nheads * headdim)\n",
    "            out = rearrange(out, 'b n h d -> b n (h d)')\n",
    "            \n",
    "        else:\n",
    "            # Use PyTorch's scaled_dot_product_attention (includes Flash Attention optimizations)\n",
    "            out = F.scaled_dot_product_attention(\n",
    "                q, k, v,\n",
    "                dropout_p=dropout_p,\n",
    "                scale=self.scale,\n",
    "                is_causal=False\n",
    "            )\n",
    "            out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "\n",
    "        return self.to_out(out)\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, config: ModelConfig):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(config.depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                FlashAttention(\n",
    "                    dim=config.dim, \n",
    "                    heads=config.heads, \n",
    "                    dim_head=config.dim_head, \n",
    "                    dropout=config.dropout, \n",
    "                    flash_attn_type=config.flash_attn_type\n",
    "                ),\n",
    "                FeedForward(config.dim, config.mlp_dim, config.dropout)\n",
    "            ]))\n",
    "    def forward(self, x):\n",
    "        for attn, ff in self.layers:\n",
    "            x = attn(x) + x\n",
    "            x = ff(x) + x\n",
    "        return x\n",
    "\n",
    "class ViT3DSegmentation(nn.Module):\n",
    "    def __init__(self, config: ModelConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        # Validate configuration\n",
    "        assert config.image_size % config.image_patch_size == 0, 'Image dimensions must be divisible by the patch size.'\n",
    "        assert config.frames % config.frame_patch_size == 0, 'Frames must be divisible by frame patch size'\n",
    "\n",
    "        # Calculate patch dimensions\n",
    "        patch_dim = config.channels * config.image_patch_size * config.image_patch_size * config.frame_patch_size\n",
    "\n",
    "        self.to_patch_embedding = nn.Sequential(\n",
    "            Rearrange('b c (f pf) (h p1) (w p2) -> b (f h w) (p1 p2 pf c)', \n",
    "                     p1 = config.image_patch_size, p2 = config.image_patch_size, pf = config.frame_patch_size),\n",
    "            nn.LayerNorm(patch_dim),\n",
    "            nn.Linear(patch_dim, config.dim),\n",
    "            nn.LayerNorm(config.dim),\n",
    "        )\n",
    "\n",
    "        # No cls token for segmentation - only positional embeddings for patches\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, config.total_patches, config.dim))\n",
    "        self.dropout = nn.Dropout(config.emb_dropout)\n",
    "\n",
    "        self.transformer = Transformer(config)\n",
    "\n",
    "        # Segmentation head - applies to each patch token\n",
    "        self.segmentation_head = nn.Sequential(\n",
    "            nn.LayerNorm(config.dim),\n",
    "            nn.Linear(config.dim, config.num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, video):\n",
    "        # video shape: (batch, channels, frames, height, width)\n",
    "        x = self.to_patch_embedding(video)\n",
    "        b, n, _ = x.shape\n",
    "\n",
    "        # Add positional embeddings (no cls token)\n",
    "        x += self.pos_embedding[:, :n]\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Process through transformer\n",
    "        x = self.transformer(x)\n",
    "\n",
    "        # Apply segmentation head to each patch token\n",
    "        x = self.segmentation_head(x)  # Shape: (batch, num_patches, num_classes)\n",
    "\n",
    "        # Reshape back to spatial dimensions\n",
    "        x = rearrange(x, 'b (f h w) c -> b c f h w', \n",
    "                     f=self.config.num_patches_f, h=self.config.num_patches_h, w=self.config.num_patches_w)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def get_patch_embeddings(self, video):\n",
    "        \"\"\"\n",
    "        Return the patch embeddings before the segmentation head\n",
    "        Useful for analysis or feature extraction\n",
    "        \"\"\"\n",
    "        x = self.to_patch_embedding(video)\n",
    "        b, n, _ = x.shape\n",
    "        x += self.pos_embedding[:, :n]\n",
    "        x = self.dropout(x)\n",
    "        x = self.transformer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8abd068b-fe31-47a2-9821-ccc68fafda07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CUDA kernels...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/torch/utils/cpp_extension.py:2080: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ CUDA kernels loaded!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "torch.backends.cuda.enable_flash_sdp(True)\n",
    "\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Rearrange\n",
    "from dataclasses import dataclass\n",
    "from typing import Tuple\n",
    "\n",
    "# helpers\n",
    "\n",
    "def pair(t):\n",
    "    return t if isinstance(t, tuple) else (t, t)\n",
    "\n",
    "def triple(t):\n",
    "    return t if isinstance(t, tuple) else (t, t, t)\n",
    "\n",
    "# Configuration classes\n",
    "\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    \"\"\"Configuration for 3D ViT model parameters\"\"\"\n",
    "    image_size: int = 64\n",
    "    image_patch_size: int = 16\n",
    "    frames: int = 16\n",
    "    frame_patch_size: int = 4\n",
    "    num_classes: int = 2\n",
    "    dim: int = 384\n",
    "    depth: int = 6\n",
    "    heads: int = 6\n",
    "    mlp_dim: int = 1024\n",
    "    channels: int = 1\n",
    "    dim_head: int = 64\n",
    "    dropout: float = 0.1\n",
    "    emb_dropout: float = 0.1\n",
    "    flash_attn_type: str = 'pytorch'\n",
    "    \n",
    "    @property\n",
    "    def num_patches_h(self):\n",
    "        return self.image_size // self.image_patch_size\n",
    "    \n",
    "    @property\n",
    "    def num_patches_w(self):\n",
    "        return self.image_size // self.image_patch_size\n",
    "    \n",
    "    @property\n",
    "    def num_patches_f(self):\n",
    "        return self.frames // self.frame_patch_size\n",
    "    \n",
    "    @property\n",
    "    def total_patches(self):\n",
    "        return self.num_patches_h * self.num_patches_w * self.num_patches_f\n",
    "    \n",
    "    @property\n",
    "    def expected_output_shape(self):\n",
    "        \"\"\"Expected output shape for given batch size\"\"\"\n",
    "        return lambda batch_size: (batch_size, self.num_classes, self.num_patches_f, self.num_patches_h, self.num_patches_w)\n",
    "\n",
    "@dataclass\n",
    "class TestConfig:\n",
    "    \"\"\"Configuration for test data and benchmarking\"\"\"\n",
    "    batch_size: int = 2\n",
    "    benchmark_iterations: int = 20\n",
    "    warmup_iterations: int = 5\n",
    "    benchmark_seq_length: int = 1024\n",
    "    # Training test parameters\n",
    "    train_steps: int = 5\n",
    "    learning_rate: float = 1e-4\n",
    "    use_mixed_precision: bool = True\n",
    "    \n",
    "    def get_test_input_shape(self, config: ModelConfig):\n",
    "        \"\"\"Get test input shape based on model config\"\"\"\n",
    "        return (self.batch_size, config.channels, config.frames, config.image_size, config.image_size)\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class FlashAttention(nn.Module):\n",
    "    def __init__(self, dim, heads = 8, dim_head = 64, dropout = 0., flash_attn_type='pytorch'):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head * heads\n",
    "        project_out = not (heads == 1 and dim_head == dim)\n",
    "\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "        self.flash_attn_type = flash_attn_type\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        ) if project_out else nn.Identity()\n",
    "\n",
    "        # Try to import flash_attn if using that backend\n",
    "        if flash_attn_type == 'flash_attn':\n",
    "            try:\n",
    "                from flash_attn import flash_attn_func\n",
    "                self.flash_attn_func = flash_attn_func\n",
    "                print(\"Using flash_attn package for attention\")\n",
    "            except ImportError:\n",
    "                print(\"flash_attn package not found, falling back to PyTorch SDPA\")\n",
    "                self.flash_attn_type = 'pytorch'\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm(x)\n",
    "        qkv = self.to_qkv(x).chunk(3, dim = -1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = self.heads), qkv)\n",
    "\n",
    "        # Determine dropout probability based on training mode\n",
    "        dropout_p = self.dropout if self.training else 0.0\n",
    "\n",
    "        if self.flash_attn_type == 'flash_attn' and hasattr(self, 'flash_attn_func'):\n",
    "            # Use dedicated flash_attn package\n",
    "            # Rearrange for flash_attn: (batch, seqlen, nheads, headdim)\n",
    "            q = rearrange(q, 'b h n d -> b n h d')\n",
    "            k = rearrange(k, 'b h n d -> b n h d')\n",
    "            v = rearrange(v, 'b h n d -> b n h d')\n",
    "            \n",
    "            out = self.flash_attn_func(\n",
    "                q, k, v,\n",
    "                dropout_p=dropout_p,\n",
    "                softmax_scale=self.scale,\n",
    "                causal=False\n",
    "            )\n",
    "            \n",
    "            # Rearrange back: (batch, seqlen, nheads, headdim) -> (batch, seqlen, nheads * headdim)\n",
    "            out = rearrange(out, 'b n h d -> b n (h d)')\n",
    "            \n",
    "        else:\n",
    "            # Use PyTorch's scaled_dot_product_attention (includes Flash Attention optimizations)\n",
    "            out = F.scaled_dot_product_attention(\n",
    "                q, k, v,\n",
    "                dropout_p=dropout_p,\n",
    "                scale=self.scale,\n",
    "                is_causal=False\n",
    "            )\n",
    "            out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "\n",
    "        return self.to_out(out)\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, config: ModelConfig):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(config.depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                FlashAttention(\n",
    "                    dim=config.dim, \n",
    "                    heads=config.heads, \n",
    "                    dim_head=config.dim_head, \n",
    "                    dropout=config.dropout, \n",
    "                    flash_attn_type=config.flash_attn_type\n",
    "                ),\n",
    "                FeedForward(config.dim, config.mlp_dim, config.dropout)\n",
    "            ]))\n",
    "    def forward(self, x):\n",
    "        for attn, ff in self.layers:\n",
    "            x = attn(x) + x\n",
    "            x = ff(x) + x\n",
    "        return x\n",
    "\n",
    "class ViT3DSegmentation(nn.Module):\n",
    "    def __init__(self, config: ModelConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        # Validate configuration\n",
    "        assert config.image_size % config.image_patch_size == 0, 'Image dimensions must be divisible by the patch size.'\n",
    "        assert config.frames % config.frame_patch_size == 0, 'Frames must be divisible by frame patch size'\n",
    "\n",
    "        # Calculate patch dimensions\n",
    "        patch_dim = config.channels * config.image_patch_size * config.image_patch_size * config.frame_patch_size\n",
    "\n",
    "        self.to_patch_embedding = nn.Sequential(\n",
    "            Rearrange('b c (f pf) (h p1) (w p2) -> b (f h w) (p1 p2 pf c)', \n",
    "                     p1 = config.image_patch_size, p2 = config.image_patch_size, pf = config.frame_patch_size),\n",
    "            nn.LayerNorm(patch_dim),\n",
    "            nn.Linear(patch_dim, config.dim),\n",
    "            nn.LayerNorm(config.dim),\n",
    "        )\n",
    "\n",
    "        # No cls token for segmentation - only positional embeddings for patches\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, config.total_patches, config.dim))\n",
    "        self.dropout = nn.Dropout(config.emb_dropout)\n",
    "\n",
    "        self.transformer = Transformer(config)\n",
    "\n",
    "        # Segmentation head - applies to each patch token\n",
    "        self.segmentation_head = nn.Sequential(\n",
    "            nn.LayerNorm(config.dim),\n",
    "            nn.Linear(config.dim, config.num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, video):\n",
    "        # video shape: (batch, channels, frames, height, width)\n",
    "        x = self.to_patch_embedding(video)\n",
    "        b, n, _ = x.shape\n",
    "\n",
    "        # Add positional embeddings (no cls token)\n",
    "        x += self.pos_embedding[:, :n]\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Process through transformer\n",
    "        x = self.transformer(x)\n",
    "\n",
    "        # Apply segmentation head to each patch token\n",
    "        x = self.segmentation_head(x)  # Shape: (batch, num_patches, num_classes)\n",
    "\n",
    "        # Reshape back to spatial dimensions\n",
    "        x = rearrange(x, 'b (f h w) c -> b c f h w', \n",
    "                     f=self.config.num_patches_f, h=self.config.num_patches_h, w=self.config.num_patches_w)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def get_patch_embeddings(self, video):\n",
    "        \"\"\"\n",
    "        Return the patch embeddings before the segmentation head\n",
    "        Useful for analysis or feature extraction\n",
    "        \"\"\"\n",
    "        x = self.to_patch_embedding(video)\n",
    "        b, n, _ = x.shape\n",
    "        x += self.pos_embedding[:, :n]\n",
    "        x = self.dropout(x)\n",
    "        x = self.transformer(x)\n",
    "        return x\n",
    "import os\n",
    "import glob\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "import numpy as np\n",
    "import tifffile\n",
    "import wandb\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "import segmentation_models_pytorch as smp\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from volumentations import Compose, Rotate, RandomCropFromBorders, ElasticTransform, Resize, Flip, RandomRotate90, GaussianNoise, RandomGamma,RandomBrightnessContrast,GridDistortion\n",
    "from scipy import ndimage as ndi\n",
    "\n",
    "\n",
    "class VesuviusFullVolumeDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for full 3D volumes from Vesuvius TIF files.\n",
    "    Each file becomes a single sample.\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset_folder, target_size=320, augment=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dataset_folder: Path to dataset folder containing imagesTr and labelsTr\n",
    "            target_size: Target size for volumes (will resize if needed)\n",
    "            augment: Whether to apply data augmentation\n",
    "        \"\"\"\n",
    "        self.dataset_folder = Path(dataset_folder)\n",
    "        self.target_size = target_size\n",
    "        self.augment = augment\n",
    "        \n",
    "        # Initialize augmentation pipeline\n",
    "        if self.augment:\n",
    "            self.aug_pipeline = self._get_augmentation(target_size)\n",
    "        \n",
    "        # Get image and label paths\n",
    "        self.images_dir = self.dataset_folder / \"imagesTr\"\n",
    "        self.labels_dir = self.dataset_folder / \"labelsTr\"\n",
    "        \n",
    "        # Find all image files and their corresponding labels\n",
    "        self.samples = []\n",
    "        image_files = list(self.images_dir.glob(\"*_0000.tif\"))\n",
    "        \n",
    "        print(f\"Found {len(image_files)} image files\")\n",
    "        \n",
    "        for img_path in image_files:\n",
    "            # Get corresponding label file\n",
    "            base_name = img_path.stem.replace(\"_0000\", \"\")\n",
    "            label_path = self.labels_dir / f\"{base_name}.tif\"\n",
    "            \n",
    "            if label_path.exists():\n",
    "                self.samples.append((img_path, label_path))\n",
    "            else:\n",
    "                print(f\"Warning: Label file not found for {img_path}\")\n",
    "        \n",
    "        print(f\"Total valid samples: {len(self.samples)}\")\n",
    "    \n",
    "    def _get_augmentation(self, patch_size):\n",
    "        \"\"\"Create volumentations augmentation pipeline\"\"\"\n",
    "        return Compose([\n",
    "            Rotate((-45, 45), (-45, 45), (-45, 45), p=0.1),\n",
    "            # RandomCropFromBorders(crop_value=0.1, p=0.5),\n",
    "            # ElasticTransform((0, 0.25), interpolation=2, p=0.1),\n",
    "            # Resize((patch_size, patch_size, patch_size), interpolation=1, resize_type=0, always_apply=True, p=1.0),\n",
    "            Flip(0, p=0.25),\n",
    "            Flip(1, p=0.25),\n",
    "            Flip(2, p=0.25),\n",
    "            RandomRotate90(p=0.25),\n",
    "\n",
    "            \n",
    "            #     RandomBrightnessContrast(brightness_limit=0.0,\n",
    "        # contrast_limit=0.0,p=.99),\n",
    "            GaussianNoise(var_limit=(0, 5), p=0.2),\n",
    "            GridDistortion(num_steps=3,p=.1),\n",
    "            # GaussianNoise(var_limit=(0, 5), p=0.2),\n",
    "            # RandomGamma(gamma_limit=(80, 120), p=0.2),\n",
    "        ], p=1.0)\n",
    "    \n",
    "    def _load_and_preprocess_volume(self, img_path, label_path):\n",
    "        \"\"\"Load and preprocess a single volume pair\"\"\"\n",
    "        # Load 3D volumes\n",
    "        image_vol = tifffile.imread(str(img_path))\n",
    "        image_vol_dtype=image_vol.dtype\n",
    "        image_vol=image_vol.astype(np.float32)\n",
    "        label_vol = tifffile.imread(str(label_path)).astype(np.float32)\n",
    "        # diamond = np.ones((2,2,2)).astype(bool)\n",
    "        # # dilate 1x with it\n",
    "        # label_vol = ndi.binary_dilation(label_vol, diamond, iterations=1)\n",
    "\n",
    "        # Ensure volumes have same shape\n",
    "        assert image_vol.shape == label_vol.shape, f\"Shape mismatch: {image_vol.shape} vs {label_vol.shape}\"\n",
    "        \n",
    "        # Resize if necessary using torch for consistency\n",
    "        if image_vol.shape[0] != self.target_size:\n",
    "            image_vol=image_vol[:self.target_size,:self.target_size,:self.target_size]\n",
    "            label_vol=label_vol[:self.target_size,:self.target_size,:self.target_size]\n",
    "\n",
    "        \n",
    "        # Normalize image\n",
    "        if image_vol_dtype==np.uint16:\n",
    "            image_vol = image_vol / 65535\n",
    "        else:\n",
    "            image_vol = image_vol / 255\n",
    "        # Ensure labels are binary\n",
    "        label_vol = (label_vol > 0).astype(np.float32)\n",
    "        \n",
    "        return image_vol, label_vol\n",
    "    \n",
    "    def _augment_volume(self, image_vol, label_vol):\n",
    "        \"\"\"Apply 3D augmentations using volumentations\"\"\"\n",
    "        if not self.augment:\n",
    "            return image_vol, label_vol\n",
    "        \n",
    "        # Convert to uint8 for volumentations (expects 0-255 range)\n",
    "        image_vol_uint8 = (image_vol * 255).astype(np.uint8)\n",
    "        label_vol_uint8 = label_vol.astype(np.uint8)\n",
    "        \n",
    "        # Apply augmentation\n",
    "        data = {'image': image_vol_uint8, 'mask': label_vol_uint8}\n",
    "        aug_data = self.aug_pipeline(**data)\n",
    "        \n",
    "        # Convert back to float32 and normalize\n",
    "        image_vol = aug_data['image'].astype(np.float32) / 255.0\n",
    "        label_vol = aug_data['mask'].astype(np.float32) \n",
    "        \n",
    "        return image_vol, label_vol\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label_path = self.samples[idx]\n",
    "        \n",
    "        # Load and preprocess\n",
    "        image_vol, label_vol = self._load_and_preprocess_volume(img_path, label_path)\n",
    "        # Apply augmentation\n",
    "        image_vol, label_vol = self._augment_volume(image_vol, label_vol)\n",
    "        # Convert to tensors\n",
    "        # plt.imshow(image_vol[0])\n",
    "        # plt.show()\n",
    "        image_tensor = torch.FloatTensor(image_vol).unsqueeze(0)  # Add channel dimension\n",
    "        label_tensor = torch.FloatTensor(label_vol)\n",
    "        \n",
    "        return image_tensor, label_tensor\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    \"\"\"Configuration for 3D ViT model parameters\"\"\"\n",
    "    image_size: int = 256\n",
    "    image_patch_size: int = 16\n",
    "    frames: int = 256\n",
    "    frame_patch_size: int = 16\n",
    "    num_classes: int = 2\n",
    "    dim: int = 512\n",
    "    depth: int = 16\n",
    "    heads: int = 16\n",
    "    mlp_dim: int = 1024\n",
    "    channels: int = 1\n",
    "    dim_head: int = 64\n",
    "    dropout: float = 0.1\n",
    "    emb_dropout: float = 0.1\n",
    "    flash_attn_type: str = 'pytorch'\n",
    "     \n",
    "    @property\n",
    "    def num_patches_h(self):\n",
    "        return self.image_size // self.image_patch_size\n",
    "    \n",
    "    @property\n",
    "    def num_patches_w(self):\n",
    "        return self.image_size // self.image_patch_size\n",
    "    \n",
    "    @property\n",
    "    def num_patches_f(self):\n",
    "        return self.frames // self.frame_patch_size\n",
    "    \n",
    "    @property\n",
    "    def total_patches(self):\n",
    "        return self.num_patches_h * self.num_patches_w * self.num_patches_f\n",
    "    \n",
    "    @property\n",
    "    def expected_output_shape(self):\n",
    "        \"\"\"Expected output shape for given batch size\"\"\"\n",
    "        return lambda batch_size: (batch_size, self.num_classes, self.num_patches_f, self.num_patches_h, self.num_patches_w)\n",
    "def load_mae_encoder_to_vit3d(mae_checkpoint_path, vit3d_model, device='cuda'):\n",
    "    checkpoint = torch.load(mae_checkpoint_path, map_location=device)\n",
    "    mae_state_dict = checkpoint['state_dict']\n",
    "    \n",
    "    vit3d_state_dict = vit3d_model.state_dict()\n",
    "    \n",
    "    loaded_keys = []\n",
    "    shape_mismatches = []\n",
    "    \n",
    "    for mae_key in mae_state_dict:\n",
    "        if mae_key.startswith('model.') and not any(x in mae_key for x in ['decoder', 'mask_token']):\n",
    "            vit3d_key = mae_key.replace('model.', '')\n",
    "            if vit3d_key in vit3d_state_dict:\n",
    "                if mae_state_dict[mae_key].shape == vit3d_state_dict[vit3d_key].shape:\n",
    "                    vit3d_state_dict[vit3d_key] = mae_state_dict[mae_key]\n",
    "                    loaded_keys.append(vit3d_key)\n",
    "                else:\n",
    "                    shape_mismatches.append((vit3d_key, mae_state_dict[mae_key].shape, vit3d_state_dict[vit3d_key].shape))\n",
    "    \n",
    "    if shape_mismatches:\n",
    "        print(f\"\\nShape mismatches found:\")\n",
    "        for key, mae_shape, vit3d_shape in shape_mismatches[:5]:\n",
    "            print(f\"{key}: MAE {mae_shape} != ViT3D {vit3d_shape}\")\n",
    "        print(f\"... and {len(shape_mismatches)-5} more\")\n",
    "    \n",
    "    vit3d_model.load_state_dict(vit3d_state_dict, strict=False)\n",
    "    return loaded_keys\n",
    "class VesuviusViT3DPLModel(pl.LightningModule):\n",
    "    def __init__(self, input_size=320, patch_size=16, num_classes=512, lr=1e-4, weight_decay=1e-4):\n",
    "        super(VesuviusViT3DPLModel, self).__init__()\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        # Add storage for first validation sample\n",
    "        self.first_val_sample = None\n",
    "        \n",
    "        # Calculate patch dimensions\n",
    "        self.patch_size = patch_size\n",
    "        self.input_size = input_size\n",
    "        self.output_size = input_size // patch_size\n",
    "        self.voxel_size = 4\n",
    "        self.intermediate_size = self.output_size * self.voxel_size\n",
    "        dim=512\n",
    "\n",
    "        self.model = ViT3DSegmentation(\n",
    "                    ModelConfig(\n",
    "            image_size=input_size,\n",
    "            image_patch_size=patch_size,\n",
    "            frames=input_size,\n",
    "            frame_patch_size=patch_size,\n",
    "            num_classes=dim,\n",
    "            dim=dim,\n",
    "            depth=16,\n",
    "            heads=16,\n",
    "            mlp_dim=1024,\n",
    "            flash_attn_type='flash_attn'\n",
    "        )\n",
    "        )\n",
    "\n",
    "        # vit3d_model = ViT3DSegmentation(config)\n",
    "        loaded = load_mae_encoder_to_vit3d('outputs/vesuvius_mae_vit3d/vesuvius_mae_vit3d_epoch=193_val_mae_loss=0.0000.ckpt', self.model)\n",
    "        # for param in self.model.parameters():\n",
    "        #     param.requires_grad = False\n",
    "        print(f\"Loaded {len(loaded)} encoder weights from MAE\")\n",
    "        # print(loaded)\n",
    "        self.token_decoder = nn.Linear(dim, self.voxel_size**3)\n",
    "        \n",
    "        self.conv_decoder = nn.Sequential(\n",
    "            nn.Conv3d(1, 64, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv3d(64, 32, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv3d(32, num_classes, 3, padding=1)\n",
    "        )\n",
    "        \n",
    "        self.loss_func1 = smp.losses.DiceLoss(mode='binary')\n",
    "        self.loss_func2 = smp.losses.SoftBCEWithLogitsLoss(smooth_factor=0.15)\n",
    "        self.loss_func = lambda x, y: 0.5 * self.loss_func1(x, y) + 0.5 * self.loss_func2(x, y)\n",
    "        # self.loss_func=self.loss_func1\n",
    "        # self.loss_func=VesuviusLoss()\n",
    "        print(f\"Model initialized:\")\n",
    "        print(f\"  Input size: {input_size}x{input_size}x{input_size}\")\n",
    "        print(f\"  Patch size: {patch_size}x{patch_size}x{patch_size}\")\n",
    "        print(f\"  Token grid: {self.output_size}x{self.output_size}x{self.output_size}\")\n",
    "        print(f\"  Intermediate size: {self.intermediate_size}x{self.intermediate_size}x{self.intermediate_size}\")\n",
    "        print(f\"  Voxel size per token: {self.voxel_size}x{self.voxel_size}x{self.voxel_size}\")\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        # with torch.no_grad():\n",
    "        embeddings = self.model(x)\n",
    "        \n",
    "        if embeddings.dim() == 5:\n",
    "            embeddings = embeddings.view(batch_size, embeddings.shape[1], -1).transpose(1, 2)\n",
    "        elif embeddings.dim() == 3:\n",
    "            pass\n",
    "        else:\n",
    "            raise ValueError(f\"Unexpected ViT output shape: {embeddings.shape}\")\n",
    "        \n",
    "        num_tokens = embeddings.shape[1]\n",
    "        expected_tokens = self.output_size ** 3\n",
    "        \n",
    "        if num_tokens != expected_tokens:\n",
    "            print(f\"Warning: Expected {expected_tokens} tokens, got {num_tokens}\")\n",
    "        \n",
    "        decoded_tokens = self.token_decoder(embeddings)\n",
    "        \n",
    "        decoded_tokens = decoded_tokens.view(\n",
    "            batch_size, \n",
    "            self.output_size, self.output_size, self.output_size,\n",
    "            self.voxel_size, self.voxel_size, self.voxel_size\n",
    "        )\n",
    "        \n",
    "        intermediate_vol = decoded_tokens.permute(0, 1, 4, 2, 5, 3, 6).contiguous()\n",
    "        intermediate_vol = intermediate_vol.view(\n",
    "            batch_size, 1, \n",
    "            self.intermediate_size, \n",
    "            self.intermediate_size, \n",
    "            self.intermediate_size\n",
    "        )\n",
    "        \n",
    "        decoded_vol = self.conv_decoder(intermediate_vol)\n",
    "        \n",
    "        output = F.interpolate(\n",
    "            decoded_vol, \n",
    "            size=(self.input_size, self.input_size, self.input_size),\n",
    "            mode='trilinear', \n",
    "            align_corners=False\n",
    "        )\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        \n",
    "        outputs = self(x)\n",
    "        \n",
    "        if y.dim() == 4:\n",
    "            y = y.unsqueeze(1)\n",
    "        \n",
    "        loss = self.loss_func(outputs, y)\n",
    "        \n",
    "        predictions = torch.sigmoid(outputs) > 0.5\n",
    "        accuracy = (predictions == (y > 0.5)).float().mean()\n",
    "        \n",
    "        intersection = (predictions * (y > 0.5)).sum()\n",
    "        union = (predictions + (y > 0.5)).clamp(0, 1).sum()\n",
    "        iou = intersection / (union + 1e-8)\n",
    "        \n",
    "        self.log(\"train/loss\", loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"train/accuracy\", accuracy, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"train/iou\", iou, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        outputs = self(x)\n",
    "        \n",
    "        if y.dim() == 4:\n",
    "            y = y.unsqueeze(1)\n",
    "        \n",
    "        loss = self.loss_func(outputs, y)\n",
    "        \n",
    "        predictions = torch.sigmoid(outputs) > 0.5\n",
    "        accuracy = (predictions == (y > 0.5)).float().mean()\n",
    "        \n",
    "        intersection = (predictions * (y > 0.5)).sum()\n",
    "        union = (predictions + (y > 0.5)).clamp(0, 1).sum()\n",
    "        iou = intersection / (union + 1e-8)\n",
    "        \n",
    "        self.log(\"val/loss\", loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"val/accuracy\", accuracy, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"val/iou\", iou, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        \n",
    "        # Store first validation sample for visualization\n",
    "        if batch_idx == 0 and self.first_val_sample is None:\n",
    "            self.first_val_sample = {\n",
    "                'image': x[0:1].cpu(),\n",
    "                'label': y[0:1].cpu(), \n",
    "                'prediction': outputs[0:1].detach().cpu()\n",
    "            }\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def visualize_3d_slices(self, image_vol, label_vol, pred_logits, pred_probs, num_slices=3):\n",
    "        d, h, w = image_vol.shape\n",
    "        slice_indices = np.linspace(0, d-1, num_slices, dtype=int)\n",
    "        \n",
    "        fig, axes = plt.subplots(4, num_slices, figsize=(12, 8))\n",
    "        if num_slices == 1:\n",
    "            axes = axes.reshape(-1, 1)\n",
    "        \n",
    "        for i, slice_idx in enumerate(slice_indices):\n",
    "            axes[0, i].imshow(image_vol[slice_idx], cmap='gray')\n",
    "            axes[0, i].set_title(f'Image Z={slice_idx}')\n",
    "            axes[0, i].axis('off')\n",
    "            \n",
    "            axes[1, i].imshow(label_vol[slice_idx], cmap='Reds', vmin=0, vmax=1)\n",
    "            axes[1, i].set_title(f'Ground Truth Z={slice_idx}')\n",
    "            axes[1, i].axis('off')\n",
    "            \n",
    "            axes[2, i].imshow(pred_logits[slice_idx], cmap='RdYlBu_r')\n",
    "            axes[2, i].set_title(f'Pred Logits Z={slice_idx}')\n",
    "            axes[2, i].axis('off')\n",
    "            \n",
    "            axes[3, i].imshow(pred_probs[slice_idx], cmap='Reds', vmin=0, vmax=1)\n",
    "            axes[3, i].set_title(f'Pred Probs Z={slice_idx}')\n",
    "            axes[3, i].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        with tempfile.NamedTemporaryFile(suffix='.png', delete=False) as tmp:\n",
    "            plt.savefig(tmp.name, dpi=100, bbox_inches='tight')\n",
    "            tmp_path = tmp.name\n",
    "        plt.close()\n",
    "        \n",
    "        return tmp_path\n",
    "    \n",
    "    def on_validation_epoch_end(self):\n",
    "        if self.first_val_sample is not None:\n",
    "            image = self.first_val_sample['image'].squeeze().numpy()\n",
    "            label = self.first_val_sample['label'].squeeze().numpy()\n",
    "            pred_logits = self.first_val_sample['prediction'].squeeze().numpy()\n",
    "            pred_probs = torch.sigmoid(self.first_val_sample['prediction']).squeeze().numpy()\n",
    "            \n",
    "            viz_path = self.visualize_3d_slices(image, label, pred_logits, pred_probs)\n",
    "            \n",
    "            if self.logger and hasattr(self.logger, 'experiment'):\n",
    "                self.logger.experiment.log({\n",
    "                    \"val/slice_visualization\": wandb.Image(viz_path)\n",
    "                })\n",
    "            \n",
    "            os.unlink(viz_path)\n",
    "            \n",
    "        self.first_val_sample = None\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        # optimizer = AdamW(list(self.token_decoder.parameters())+list(self.conv_decoder.parameters()), lr=self.hparams.lr, weight_decay=self.hparams.weight_decay)\n",
    "        optimizer = AdamW(self.parameters(), lr=self.hparams.lr, weight_decay=self.hparams.weight_decay)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=35, eta_min=1e-6)\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "\n",
    "class CFG:\n",
    "    \"\"\"Configuration class\"\"\"\n",
    "    # Dataset\n",
    "    dataset_folder = './Dataset090_CEDCrops/'  # Change this to your dataset path\n",
    "    \n",
    "    # Model\n",
    "    input_size = 256      # Size of input volumes\n",
    "    patch_size = 16       # Patch size for ViT\n",
    "    num_classes = 1       # Binary segmentation\n",
    "    \n",
    "    # Training\n",
    "    batch_size = 2        # Small batch size due to large volumes\n",
    "    num_workers = 32      # Reduced workers for large volumes\n",
    "    lr = 3e-4\n",
    "    weight_decay = 2e-6\n",
    "    epochs = 500\n",
    "    \n",
    "    # Data split\n",
    "    train_ratio = 0.99\n",
    "    \n",
    "    # Experiment\n",
    "    exp_name = 'vesuvius_vit3d_full_volume'\n",
    "    \n",
    "    # Paths\n",
    "    model_dir = './outputs/vesuvius_vit3d/'\n",
    "    \n",
    "    def __init__(self):\n",
    "        os.makedirs(self.model_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "def split_dataset(dataset, train_ratio=0.8):\n",
    "    \"\"\"Split dataset into train and validation\"\"\"\n",
    "    num_samples = len(dataset)\n",
    "    num_train = int(num_samples * train_ratio)\n",
    "    \n",
    "    indices = torch.randperm(num_samples)\n",
    "    train_indices = indices[:num_train]\n",
    "    val_indices = indices[num_train:]\n",
    "    \n",
    "    train_dataset = torch.utils.data.Subset(dataset, train_indices)\n",
    "    val_dataset = torch.utils.data.Subset(dataset, val_indices)\n",
    "    \n",
    "    return train_dataset, val_dataset\n",
    "\n",
    "\n",
    "def count_dataset_stats(dataset_folder):\n",
    "    \"\"\"Count basic statistics about the dataset\"\"\"\n",
    "    dataset_folder = Path(dataset_folder)\n",
    "    images_dir = dataset_folder / \"imagesTr\"\n",
    "    labels_dir = dataset_folder / \"labelsTr\"\n",
    "    \n",
    "    image_files = list(images_dir.glob(\"*_0000.tif\"))\n",
    "    print(f\"Found {len(image_files)} image files\")\n",
    "    \n",
    "    # Sample a few files to check shapes and statistics\n",
    "    for i, img_path in enumerate(image_files[:3]):\n",
    "        base_name = img_path.stem.replace(\"_0000\", \"\")\n",
    "        label_path = labels_dir / f\"{base_name}.tif\"\n",
    "        \n",
    "        if label_path.exists():\n",
    "            try:\n",
    "                image_vol = tifffile.imread(str(img_path))\n",
    "                label_vol = tifffile.imread(str(label_path))\n",
    "                \n",
    "                positive_ratio = (label_vol > 0).mean()\n",
    "                print(f\"  {base_name}:\")\n",
    "                print(f\"    Image shape: {image_vol.shape}\")\n",
    "                print(f\"    Label shape: {label_vol.shape}\")\n",
    "                print(f\"    Image range: [{image_vol.min()}, {image_vol.max()}]\")\n",
    "                print(f\"    Positive ratio: {positive_ratio:.4f}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"    Error reading {img_path}: {e}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main training function\"\"\"\n",
    "    cfg = CFG()\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"Vesuvius 3D ViT Training Pipeline - Full Volumes\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Check dataset\n",
    "    if not os.path.exists(cfg.dataset_folder):\n",
    "        print(f\"Error: Dataset folder not found at {cfg.dataset_folder}\")\n",
    "        print(\"Please update CFG.dataset_folder to point to your dataset\")\n",
    "        return\n",
    "    \n",
    "    # Count dataset statistics\n",
    "    print(\"Dataset statistics:\")\n",
    "    count_dataset_stats(cfg.dataset_folder)\n",
    "    \n",
    "    # Create full dataset\n",
    "    print(f\"\\nCreating dataset with target size {cfg.input_size}...\")\n",
    "    full_dataset = VesuviusFullVolumeDataset(\n",
    "        dataset_folder=cfg.dataset_folder,\n",
    "        target_size=cfg.input_size,\n",
    "        augment=True\n",
    "    )\n",
    "    \n",
    "    # Split into train and validation\n",
    "    train_dataset, val_dataset = split_dataset(full_dataset, cfg.train_ratio)\n",
    "    # val_dataset.augment=False\n",
    "    print(f\"Training samples: {len(train_dataset)}\")\n",
    "    print(f\"Validation samples: {len(val_dataset)}\")\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=cfg.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=cfg.num_workers,\n",
    "        pin_memory=True,\n",
    "        drop_last=True\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=cfg.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=cfg.num_workers,\n",
    "        pin_memory=True,\n",
    "        drop_last=False\n",
    "    ) if len(val_dataset) > 0 else None\n",
    "    \n",
    "    # Create model\n",
    "    # vesuvius_vit3d_12l_8h_512_flashattn_epoch=250_val_loss=0.0000.ckpt\n",
    "    model = VesuviusViT3DPLModel(\n",
    "        input_size=cfg.input_size,\n",
    "        patch_size=cfg.patch_size,\n",
    "        num_classes=cfg.num_classes,\n",
    "        lr=cfg.lr,\n",
    "        weight_decay=cfg.weight_decay\n",
    "    )\n",
    "    \n",
    "    # Print model info\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"\\nModel parameters: {total_params:,}\")\n",
    "    \n",
    "    # Memory estimate\n",
    "    batch_memory_gb = (cfg.batch_size * 1 * cfg.input_size**3 * 4) / (1024**3)  # 4 bytes per float32\n",
    "    print(f\"Estimated memory per batch: {batch_memory_gb:.2f} GB\")\n",
    "    \n",
    "    # Setup logging\n",
    "    wandb_logger = WandbLogger(\n",
    "        project=\"vesuvius_vit3d\",\n",
    "        name=f\"{cfg.exp_name}_size_{cfg.input_size}_patch_{cfg.patch_size}\"\n",
    "    )\n",
    "    \n",
    "    # Setup callbacks\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        dirpath=cfg.model_dir,\n",
    "        filename=f'vesuvius_vit3d_pretrainedMAE_surface_flashattn_{{epoch:02d}}_{{val_loss:.4f}}',\n",
    "        monitor='val/loss' if val_loader else 'train/loss',\n",
    "        mode='min',\n",
    "        save_top_k=8,\n",
    "        save_last=True\n",
    "    )\n",
    "    \n",
    "    # Create trainer\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=cfg.epochs,\n",
    "        accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n",
    "        devices=1,\n",
    "        logger=wandb_logger,\n",
    "        callbacks=[checkpoint_callback],\n",
    "        precision='16-mixed' if torch.cuda.is_available() else 32,\n",
    "        gradient_clip_val=5.0,\n",
    "        log_every_n_steps=10,\n",
    "        accumulate_grad_batches=12,  # Gradient accumulation for larger effective batch size\n",
    "        # check_val_every_n_epoch=3   # Validate less frequently\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    print(f\"\\nStarting training for {cfg.epochs} epochs...\")\n",
    "    print(f\"Effective batch size: {cfg.batch_size * 4}\")  # Due to gradient accumulation\n",
    "    \n",
    "    trainer.fit(model, train_loader, val_loader)\n",
    "    \n",
    "    print(\"Training completed!\")\n",
    "    wandb.finish()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
