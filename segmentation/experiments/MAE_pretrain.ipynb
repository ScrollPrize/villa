{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75dfcce5-44e0-41c5-b4c9-15d7bbcc8baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from functools import partial\n",
    "from dataclasses import dataclass\n",
    "from typing import Tuple\n",
    "import numpy as np\n",
    "from pytorch_optimizer import load_optimizer\n",
    "\n",
    "# Enable Flash Attention optimizations\n",
    "torch.backends.cuda.enable_flash_sdp(True)\n",
    "\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "# helpers\n",
    "def pair(t):\n",
    "    return t if isinstance(t, tuple) else (t, t)\n",
    "\n",
    "def triple(t):\n",
    "    return t if isinstance(t, tuple) else (t, t, t)\n",
    "\n",
    "# Configuration classes\n",
    "\n",
    "@dataclass\n",
    "class MAEConfig:\n",
    "    \"\"\"Centralized configuration for ViT3D MAE with ViT3DSegmentation components\"\"\"\n",
    "    # Model architecture\n",
    "    image_size: int = 256\n",
    "    image_patch_size: int = 16\n",
    "    frames: int = 256\n",
    "    frame_patch_size: int = 16\n",
    "    channels: int = 1\n",
    "    \n",
    "    # Encoder (matching ViT3DSegmentation)\n",
    "    dim: int = 1024\n",
    "    depth: int = 12\n",
    "    heads: int = 8\n",
    "    dim_head: int = 64\n",
    "    mlp_dim: int = 1024\n",
    "    dropout: float = 0.1\n",
    "    emb_dropout: float = 0.1\n",
    "    flash_attn_type: str = 'flash_attn'  # 'pytorch' or 'flash_attn'\n",
    "    \n",
    "    # Decoder\n",
    "    decoder_dim: int = 512\n",
    "    decoder_depth: int = 4\n",
    "    decoder_heads: int = 8\n",
    "    \n",
    "    # Training\n",
    "    mask_ratio: float = 0.75\n",
    "    loss_type: str = 'mse'\n",
    "    norm_pix_loss: bool = False\n",
    "    \n",
    "    # ViT3DSegmentation decoder parameters\n",
    "    voxel_size: int = 4\n",
    "    \n",
    "    @property\n",
    "    def num_patches_h(self):\n",
    "        return self.image_size // self.image_patch_size\n",
    "    \n",
    "    @property\n",
    "    def num_patches_w(self):\n",
    "        return self.image_size // self.image_patch_size\n",
    "    \n",
    "    @property\n",
    "    def num_patches_f(self):\n",
    "        return self.frames // self.frame_patch_size\n",
    "    \n",
    "    @property\n",
    "    def total_patches(self):\n",
    "        return self.num_patches_h * self.num_patches_w * self.num_patches_f\n",
    "    \n",
    "    @property\n",
    "    def patch_dim(self):\n",
    "        return self.channels * self.image_patch_size * self.image_patch_size * self.frame_patch_size\n",
    "    \n",
    "    @property\n",
    "    def output_size(self):\n",
    "        return self.image_size // self.image_patch_size\n",
    "    \n",
    "    @property\n",
    "    def intermediate_size(self):\n",
    "        return self.output_size * self.voxel_size\n",
    "\n",
    "# Flash Attention Module (same as ViT3DSegmentation)\n",
    "class FlashAttention(nn.Module):\n",
    "    def __init__(self, dim, heads=8, dim_head=64, dropout=0., flash_attn_type='pytorch'):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head * heads\n",
    "        project_out = not (heads == 1 and dim_head == dim)\n",
    "\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "        self.flash_attn_type = flash_attn_type\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias=False)\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        ) if project_out else nn.Identity()\n",
    "\n",
    "        # Try to import flash_attn if using that backend\n",
    "        if flash_attn_type == 'flash_attn':\n",
    "            try:\n",
    "                from flash_attn import flash_attn_func\n",
    "                self.flash_attn_func = flash_attn_func\n",
    "                print(\"Using flash_attn package for attention\")\n",
    "            except ImportError:\n",
    "                print(\"flash_attn package not found, falling back to PyTorch SDPA\")\n",
    "                self.flash_attn_type = 'pytorch'\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm(x)\n",
    "        qkv = self.to_qkv(x).chunk(3, dim=-1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h=self.heads), qkv)\n",
    "\n",
    "        # Determine dropout probability based on training mode\n",
    "        dropout_p = self.dropout if self.training else 0.0\n",
    "\n",
    "        if self.flash_attn_type == 'flash_attn' and hasattr(self, 'flash_attn_func'):\n",
    "            # Use dedicated flash_attn package\n",
    "            q = rearrange(q, 'b h n d -> b n h d')\n",
    "            k = rearrange(k, 'b h n d -> b n h d')\n",
    "            v = rearrange(v, 'b h n d -> b n h d')\n",
    "            \n",
    "            out = self.flash_attn_func(\n",
    "                q, k, v,\n",
    "                dropout_p=dropout_p,\n",
    "                softmax_scale=self.scale,\n",
    "                causal=False\n",
    "            )\n",
    "            \n",
    "            out = rearrange(out, 'b n h d -> b n (h d)')\n",
    "            \n",
    "        else:\n",
    "            # Use PyTorch's scaled_dot_product_attention (includes Flash Attention optimizations)\n",
    "            out = F.scaled_dot_product_attention(\n",
    "                q, k, v,\n",
    "                dropout_p=dropout_p,\n",
    "                scale=self.scale,\n",
    "                is_causal=False\n",
    "            )\n",
    "            out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "\n",
    "        return self.to_out(out)\n",
    "\n",
    "# Feed Forward Module (same as ViT3DSegmentation)\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, dropout=0.):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# Transformer Block (same as ViT3DSegmentation)\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config: MAEConfig):\n",
    "        super().__init__()\n",
    "        self.attention = FlashAttention(\n",
    "            dim=config.dim,\n",
    "            heads=config.heads,\n",
    "            dim_head=config.dim_head,\n",
    "            dropout=config.dropout,\n",
    "            flash_attn_type=config.flash_attn_type\n",
    "        )\n",
    "        self.feed_forward = FeedForward(config.dim, config.mlp_dim, config.dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.attention(x) + x\n",
    "        x = self.feed_forward(x) + x\n",
    "        return x\n",
    "\n",
    "# Transformer (same as ViT3DSegmentation)\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, config: MAEConfig):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(config.depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                FlashAttention(\n",
    "                    dim=config.dim, \n",
    "                    heads=config.heads, \n",
    "                    dim_head=config.dim_head, \n",
    "                    dropout=config.dropout, \n",
    "                    flash_attn_type=config.flash_attn_type\n",
    "                ),\n",
    "                FeedForward(config.dim, config.mlp_dim, config.dropout)\n",
    "            ]))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for attn, ff in self.layers:\n",
    "            x = attn(x) + x\n",
    "            x = ff(x) + x\n",
    "        return x\n",
    "\n",
    "# Main 3D MAE Model with ViT3DSegmentation Encoder and Decoder\n",
    "class ViT3DMAE_WithViTDecoder(nn.Module):\n",
    "    \"\"\" 3D Masked Autoencoder using ViT3DSegmentation encoder and decoder \"\"\"\n",
    "    \n",
    "    def __init__(self, config: MAEConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        # --------------------------------------------------------------------------\n",
    "        # ENCODER: EXACT MATCH WITH ViT3DSegmentation\n",
    "        # --------------------------------------------------------------------------\n",
    "        \n",
    "        # Calculate patch dimensions\n",
    "        patch_dim = config.channels * config.image_patch_size * config.image_patch_size * config.frame_patch_size\n",
    "        \n",
    "        # Same patch embedding as ViT3DSegmentation\n",
    "        self.to_patch_embedding = nn.Sequential(\n",
    "            Rearrange('b c (f pf) (h p1) (w p2) -> b (f h w) (p1 p2 pf c)', \n",
    "                     p1 = config.image_patch_size, p2 = config.image_patch_size, pf = config.frame_patch_size),\n",
    "            nn.LayerNorm(patch_dim),\n",
    "            nn.Linear(patch_dim, config.dim),\n",
    "            nn.LayerNorm(config.dim),\n",
    "        )\n",
    "\n",
    "        # Same positional embedding as ViT3DSegmentation (no cls token for segmentation)\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, config.total_patches, config.dim))\n",
    "        self.dropout = nn.Dropout(config.emb_dropout)\n",
    "\n",
    "        # Same transformer as ViT3DSegmentation\n",
    "        self.transformer = Transformer(config)\n",
    "        # --------------------------------------------------------------------------\n",
    "\n",
    "        # --------------------------------------------------------------------------\n",
    "        # MAE DECODER: Attention layers + ViT3DSegmentation decoder\n",
    "        # --------------------------------------------------------------------------\n",
    "        \n",
    "        # Decoder embedding to project encoder output to decoder dimension\n",
    "        self.decoder_embed = nn.Linear(config.dim, config.decoder_dim, bias=True)\n",
    "\n",
    "        # Mask token for missing patches\n",
    "        self.mask_token = nn.Parameter(torch.zeros(1, 1, config.decoder_dim))\n",
    "\n",
    "        # Decoder positional embeddings\n",
    "        self.decoder_pos_embed = nn.Parameter(torch.randn(1, config.total_patches, config.decoder_dim))\n",
    "        # print(self.pos_embedding.requires_grad,self.mask_token.requires_grad)\n",
    "        # Create decoder attention blocks\n",
    "        decoder_config = MAEConfig()\n",
    "        decoder_config.dim = config.decoder_dim\n",
    "        decoder_config.depth = config.decoder_depth\n",
    "        decoder_config.heads = config.decoder_heads\n",
    "        decoder_config.dim_head = config.dim_head\n",
    "        decoder_config.mlp_dim = config.decoder_dim * 4\n",
    "        decoder_config.dropout = config.dropout\n",
    "        decoder_config.flash_attn_type = config.flash_attn_type\n",
    "\n",
    "        self.decoder_blocks = nn.ModuleList([])\n",
    "        for _ in range(config.decoder_depth):\n",
    "            self.decoder_blocks.append(nn.ModuleList([\n",
    "                FlashAttention(\n",
    "                    dim=config.decoder_dim, \n",
    "                    heads=config.decoder_heads, \n",
    "                    dim_head=config.dim_head, \n",
    "                    dropout=config.dropout, \n",
    "                    flash_attn_type=config.flash_attn_type\n",
    "                ),\n",
    "                FeedForward(config.decoder_dim, config.decoder_dim * 4, config.dropout)\n",
    "            ]))\n",
    "\n",
    "        self.decoder_norm = nn.LayerNorm(config.decoder_dim)\n",
    "        # --------------------------------------------------------------------------\n",
    "        # ViT3DSegmentation DECODER: Exact match\n",
    "        # --------------------------------------------------------------------------\n",
    "        # Token decoder: converts tokens to voxel representations\n",
    "        self.token_decoder = nn.Linear(config.decoder_dim, config.voxel_size**3)\n",
    "        # \n",
    "        # Conv decoder: processes voxel volume to final output\n",
    "        self.conv_decoder = nn.Sequential(\n",
    "            nn.Conv3d(1, 64, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv3d(64, 32, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv3d(32, 1, 3, padding=1)  # Output 1 channel for reconstruction\n",
    "        )\n",
    "        # --------------------------------------------------------------------------\n",
    "\n",
    "        self.norm_pix_loss = config.norm_pix_loss\n",
    "\n",
    "        self.initialize_weights()\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        # Initialize tokens with small random values\n",
    "        torch.nn.init.normal_(self.mask_token, std=.02)\n",
    "        \n",
    "        # Initialize positional embeddings with small random values\n",
    "        torch.nn.init.normal_(self.pos_embedding, std=.02)\n",
    "        torch.nn.init.normal_(self.decoder_pos_embed, std=.02)\n",
    "\n",
    "        # Initialize nn.Linear and nn.LayerNorm\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    def random_masking(self, x, mask_ratio):\n",
    "        \"\"\"\n",
    "        Perform per-sample random masking by per-sample shuffling.\n",
    "        \"\"\"\n",
    "        N, L, D = x.shape\n",
    "        len_keep = int(L * (1 - mask_ratio))\n",
    "        \n",
    "        noise = torch.rand(N, L, device=x.device)\n",
    "        \n",
    "        ids_shuffle = torch.argsort(noise, dim=1)\n",
    "        ids_restore = torch.argsort(ids_shuffle, dim=1)\n",
    "\n",
    "        ids_keep = ids_shuffle[:, :len_keep]\n",
    "        x_masked = torch.gather(x, dim=1, index=ids_keep.unsqueeze(-1).repeat(1, 1, D))\n",
    "\n",
    "        mask = torch.ones([N, L], device=x.device)\n",
    "        mask[:, :len_keep] = 0\n",
    "        mask = torch.gather(mask, dim=1, index=ids_restore)\n",
    "\n",
    "        return x_masked, mask, ids_restore\n",
    "\n",
    "    def forward_encoder(self, x, mask_ratio):\n",
    "        \"\"\"Forward pass through encoder - EXACT MATCH with ViT3DSegmentation\"\"\"\n",
    "        # Patch embedding (same as ViT3DSegmentation)\n",
    "        x = self.to_patch_embedding(x)\n",
    "        b, n, _ = x.shape\n",
    "\n",
    "        # Add positional embeddings (no cls token, same as ViT3DSegmentation)\n",
    "        x = x + self.pos_embedding[:, :n]\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Apply masking\n",
    "        x, mask, ids_restore = self.random_masking(x, mask_ratio)\n",
    "\n",
    "        # Apply transformer (same as ViT3DSegmentation)\n",
    "        for attn, ff in self.transformer.layers:\n",
    "            x = attn(x) + x\n",
    "            x = ff(x) + x\n",
    "\n",
    "        return x, mask, ids_restore\n",
    "\n",
    "    def forward_decoder(self, x, ids_restore):\n",
    "        \"\"\"Forward pass through decoder - Attention layers + ViT3DSegmentation decoder\"\"\"\n",
    "        \n",
    "        # Embed tokens to decoder dimension\n",
    "        x = self.decoder_embed(x)\n",
    "\n",
    "        # Append mask tokens to sequence\n",
    "        mask_tokens = self.mask_token.repeat(x.shape[0], ids_restore.shape[1] - x.shape[1], 1)\n",
    "        x_ = torch.cat([x, mask_tokens], dim=1)  # no cls token\n",
    "        x_ = torch.gather(x_, dim=1, index=ids_restore.unsqueeze(-1).repeat(1, 1, x.shape[2]))  # unshuffle\n",
    "\n",
    "        # Add positional embeddings\n",
    "        x = x_ + self.pos_embedding\n",
    "\n",
    "        # Apply decoder attention blocks\n",
    "        for attn, ff in self.decoder_blocks:\n",
    "            x = attn(x) + x\n",
    "            x = ff(x) + x\n",
    "        x = self.decoder_norm(x)\n",
    "\n",
    "        # --------------------------------------------------------------------------\n",
    "        # ViT3DSegmentation Decoder Components\n",
    "        # --------------------------------------------------------------------------\n",
    "        \n",
    "        batch_size = x.shape[0]\n",
    "        \n",
    "        # Apply token decoder: convert each patch token to voxel representation\n",
    "        decoded_tokens = self.token_decoder(x)  # (B, num_patches, voxel_size^3)\n",
    "        \n",
    "        # Reshape tokens to spatial 3D arrangement\n",
    "        decoded_tokens = decoded_tokens.view(\n",
    "            batch_size, \n",
    "            self.config.output_size, self.config.output_size, self.config.output_size,\n",
    "            self.config.voxel_size, self.config.voxel_size, self.config.voxel_size\n",
    "        )\n",
    "        \n",
    "        # Rearrange to create intermediate volume\n",
    "        intermediate_vol = decoded_tokens.permute(0, 1, 4, 2, 5, 3, 6).contiguous()\n",
    "        intermediate_vol = intermediate_vol.view(\n",
    "            batch_size, 1, \n",
    "            self.config.intermediate_size, \n",
    "            self.config.intermediate_size, \n",
    "            self.config.intermediate_size\n",
    "        )\n",
    "        \n",
    "        # Apply conv decoder to process the intermediate volume\n",
    "        decoded_vol = self.conv_decoder(intermediate_vol)\n",
    "        \n",
    "        # Interpolate back to input size\n",
    "        output = F.interpolate(\n",
    "            decoded_vol, \n",
    "            size=(self.config.frames, self.config.image_size, self.config.image_size),\n",
    "            mode='trilinear', \n",
    "            align_corners=False\n",
    "        )\n",
    "        \n",
    "        return output\n",
    "\n",
    "    def patch_mask_to_volume_mask(self, patch_mask):\n",
    "        \"\"\"Convert patch-level mask to volume-level mask\"\"\"\n",
    "        B, L = patch_mask.shape\n",
    "        \n",
    "        # Reshape patch mask to spatial dimensions\n",
    "        mask_3d = patch_mask.view(B, self.config.num_patches_f, self.config.num_patches_h, self.config.num_patches_w)\n",
    "        \n",
    "        # Expand each patch to its corresponding voxel region\n",
    "        mask_vol = mask_3d.unsqueeze(1)  # Add channel dimension\n",
    "        mask_vol = F.interpolate(\n",
    "            mask_vol.float(), \n",
    "            size=(self.config.frames, self.config.image_size, self.config.image_size),\n",
    "            mode='nearest'\n",
    "        )\n",
    "        \n",
    "        return mask_vol\n",
    "\n",
    "    def compute_loss(self, video, pred=None, mask=None, mask_ratio=0.75, \n",
    "                    loss_type='huber', norm_pix_loss=False, return_components=False):\n",
    "        \"\"\"\n",
    "        Compute reconstruction loss on full volume (not patches)\n",
    "        \"\"\"\n",
    "        # Run forward pass if predictions not provided\n",
    "        if pred is None or mask is None:\n",
    "            _, pred, mask = self.forward(video, mask_ratio)\n",
    "        \n",
    "        # Target is the original video\n",
    "        target = video\n",
    "        \n",
    "        # Convert patch mask to volume mask for loss computation\n",
    "        volume_mask = self.patch_mask_to_volume_mask(mask)\n",
    "        \n",
    "        # Normalize target if requested\n",
    "        if norm_pix_loss:\n",
    "            mean = target.mean(dim=[2, 3, 4], keepdim=True)\n",
    "            var = target.var(dim=[2, 3, 4], keepdim=True)\n",
    "            target = (target - mean) / (var + 1e-6)**0.5\n",
    "        \n",
    "        # Compute loss based on type\n",
    "        if loss_type == 'mse':\n",
    "            loss = (pred - target) ** 2\n",
    "        elif loss_type == 'l1':\n",
    "            loss = torch.abs(pred - target)\n",
    "        elif loss_type == 'smooth_l1':\n",
    "            loss = torch.nn.functional.smooth_l1_loss(pred, target, reduction='none')\n",
    "        elif loss_type == 'huber':\n",
    "            loss = torch.nn.functional.huber_loss(pred, target, reduction='none', delta=1.0)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown loss type: {loss_type}\")\n",
    "        \n",
    "        # Apply mask - only compute loss on masked regions\n",
    "        masked_loss = (loss * volume_mask).sum() / (volume_mask.sum() + 1e-8)\n",
    "        \n",
    "        # Also compute loss on visible regions for monitoring\n",
    "        visible_loss = (loss * (1 - volume_mask)).sum() / ((1 - volume_mask).sum() + 1e-8)\n",
    "        \n",
    "        if return_components:\n",
    "            components = {\n",
    "                'total_loss': masked_loss,\n",
    "                'masked_loss': masked_loss,\n",
    "                'visible_loss': visible_loss,\n",
    "                'mask_ratio': mask.mean(),\n",
    "                'pred_std': pred.std(),\n",
    "                'target_std': target.std()\n",
    "            }\n",
    "            return masked_loss, components\n",
    "        \n",
    "        return masked_loss\n",
    "\n",
    "    def forward(self, video, mask_ratio=0.75, loss_type='mse', norm_pix_loss=False):\n",
    "        \"\"\"MAE forward pass with ViT3DSegmentation components\"\"\"\n",
    "        latent, mask, ids_restore = self.forward_encoder(video, mask_ratio)\n",
    "        pred = self.forward_decoder(latent, ids_restore)\n",
    "        loss = self.compute_loss(video, pred, mask, mask_ratio, loss_type, norm_pix_loss=False)\n",
    "        return loss, pred, mask\n",
    "\n",
    "    def forward_encoder_only(self, video, mask_ratio=0.0):\n",
    "        \"\"\"For feature extraction without reconstruction\"\"\"\n",
    "        return self.forward_encoder(video, mask_ratio)\n",
    "\n",
    "    def get_patch_embeddings(self, video):\n",
    "        \"\"\"\n",
    "        Return the patch embeddings before masking (same as ViT3DSegmentation)\n",
    "        Useful for analysis or feature extraction\n",
    "        \"\"\"\n",
    "        x = self.to_patch_embedding(video)\n",
    "        b, n, _ = x.shape\n",
    "        x = x + self.pos_embedding[:, :n]\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Apply transformer (same as ViT3DSegmentation)\n",
    "        for attn, ff in self.transformer.layers:\n",
    "            x = attn(x) + x\n",
    "            x = ff(x) + x\n",
    "            \n",
    "        return x\n",
    "\n",
    "\n",
    "# Create a convenience function to match the original MAE config creation\n",
    "def create_config_vesuvius(input_size=256, patch_size=16, mask_ratio=0.8):\n",
    "    \"\"\"Create MAE config matching ViT3DSegmentation parameters\"\"\"\n",
    "    return MAEConfig(\n",
    "        image_size=input_size,\n",
    "        image_patch_size=patch_size,\n",
    "        frames=input_size,\n",
    "        frame_patch_size=patch_size,\n",
    "        channels=1,\n",
    "        dim=512,\n",
    "        depth=16,\n",
    "        heads=16,\n",
    "        dim_head=64,\n",
    "        mlp_dim=1024,\n",
    "        dropout=0.1,\n",
    "        emb_dropout=0.1,\n",
    "        flash_attn_type='flash_attn',\n",
    "        decoder_dim=512,\n",
    "        decoder_depth=6,\n",
    "        decoder_heads=12,\n",
    "        mask_ratio=mask_ratio,\n",
    "        voxel_size=8\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "675df389-f7bd-4d23-8b1d-2f21fcac5f6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset statistics:\n",
      "Found 47896 image files\n",
      "  chunk_43_17_12:\n",
      "    Image shape: (256, 256, 256)\n",
      "    Image range: [0, 255]\n",
      "  chunk_2_19_21:\n",
      "    Image shape: (256, 256, 256)\n",
      "    Image range: [0, 255]\n",
      "  1451_chunk_14_10_14:\n",
      "    Image shape: (256, 256, 256)\n",
      "    Image range: [0, 255]\n",
      "\n",
      "Creating MAE dataset with target size 256...\n",
      "Found 47896 image files\n",
      "Total samples for MAE pretraining: 47896\n",
      "Training samples: 47417\n",
      "Validation samples: 479\n",
      "Using flash_attn package for attention\n",
      "Using flash_attn package for attention\n",
      "Using flash_attn package for attention\n",
      "Using flash_attn package for attention\n",
      "Using flash_attn package for attention\n",
      "Using flash_attn package for attention\n",
      "Using flash_attn package for attention\n",
      "Using flash_attn package for attention\n",
      "Using flash_attn package for attention\n",
      "Using flash_attn package for attention\n",
      "Using flash_attn package for attention\n",
      "Using flash_attn package for attention\n",
      "Using flash_attn package for attention\n",
      "Using flash_attn package for attention\n",
      "Using flash_attn package for attention\n",
      "Using flash_attn package for attention\n",
      "Using flash_attn package for attention\n",
      "Using flash_attn package for attention\n",
      "Using flash_attn package for attention\n",
      "Using flash_attn package for attention\n",
      "Using flash_attn package for attention\n",
      "Using flash_attn package for attention\n",
      "Modified MAE Model initialized:\n",
      "  Input size: 256x256x256\n",
      "  Patch size: 16x16x16\n",
      "  Mask ratio: 0.75\n",
      "  Encoder dim: 512\n",
      "  Decoder dim: 512\n",
      "  Using ViT3DSegmentation encoder and decoder components\n",
      "\n",
      "Modified MAE Model parameters: 79,334,017\n",
      "Estimated memory per batch: 0.50 GB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.20.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./wandb/run-20250725_153955-9y3pwj3c</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/joenader/vesuvius_mae_vit3d_hybrid/runs/9y3pwj3c' target=\"_blank\">vesuvius_mae_vit3d_hybrid_size_256_patch_16_mask_0.75</a></strong> to <a href='https://wandb.ai/joenader/vesuvius_mae_vit3d_hybrid' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/joenader/vesuvius_mae_vit3d_hybrid' target=\"_blank\">https://wandb.ai/joenader/vesuvius_mae_vit3d_hybrid</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/joenader/vesuvius_mae_vit3d_hybrid/runs/9y3pwj3c' target=\"_blank\">https://wandb.ai/joenader/vesuvius_mae_vit3d_hybrid/runs/9y3pwj3c</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "/home/ubuntu/.local/lib/python3.10/site-packages/pytorch_lightning/plugins/precision/amp.py:53: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type                    | Params\n",
      "--------------------------------------------------\n",
      "0 | model | ViT3DMAE_WithViTDecoder | 79.3 M\n",
      "--------------------------------------------------\n",
      "79.3 M    Trainable params\n",
      "0         Non-trainable params\n",
      "79.3 M    Total params\n",
      "317.336   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting modified MAE pretraining for 500 epochs...\n",
      "Mask ratio: 0.75\n",
      "Using ViT3DSegmentation encoder and decoder components\n",
      "Effective batch size: 64\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ee46bc5928b4335908b9fe2132c4f48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21a1b3176e1b4a328cac63bd86f6819a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1aae67bffe3e4b55b0f6a03de19f637c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb129540f58f4ad0b9a4a6d5f960cb4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab56cf54c79e453b9247034309c743c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00bb116f5b0c44c2b8411eacaedd635c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "505641eb51454ac8bd0aa18c1b35d135",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7aeb0d3e1b904198bb5ba318bc008167",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69fb853171c14d99afcafdf97f606b27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffc052fe7c5f49cba7e35418941473cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modified MAE pretraining completed!\n",
      "Saved model checkpoints in: ./outputs/vesuvius_mae_vit3d/\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-37 (_pin_memory_loop):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/threading.py\", line 953, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3/dist-packages/torch/utils/data/_utils/pin_memory.py\", line 59, in _pin_memory_loop\n",
      "    do_one_step()\n",
      "  File \"/usr/lib/python3/dist-packages/torch/utils/data/_utils/pin_memory.py\", line 35, in do_one_step\n",
      "    r = in_queue.get(timeout=MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 122, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/usr/lib/python3/dist-packages/torch/multiprocessing/reductions.py\", line 541, in rebuild_storage_fd\n",
      "    fd = df.detach()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/resource_sharer.py\", line 57, in detach\n",
      "    with _resource_sharer.get_connection(self._id) as conn:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/resource_sharer.py\", line 86, in get_connection\n",
      "    c = Client(address, authkey=process.current_process().authkey)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 502, in Client\n",
      "    c = SocketClient(address)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 630, in SocketClient\n",
      "    s.connect(address)\n",
      "FileNotFoundError: [Errno 2] No such file or directory\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▁▁▁▂▂▂▂▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▅▅▅▆▆▆▆▇▇▇███</td></tr><tr><td>train/mae_loss_epoch</td><td>█▄▃▃▂▂▁▁</td></tr><tr><td>train/mae_loss_step</td><td>█▅▇▇▄▄▂▃▅▅▂▃▁▃▃▃▂▂▃▃▂▄▁▂▂▂▂▂▁▂▃▂▂▂▂▃▃▁▂▂</td></tr><tr><td>train/mask_ratio_epoch</td><td>▁▁▁▁▁▁▁▁</td></tr><tr><td>train/mask_ratio_step</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/masked_error_epoch</td><td>█▄▃▃▂▂▁▁</td></tr><tr><td>train/masked_error_step</td><td>█▆▇▄▄▃▁▄▄▁▃▂▃▂▁▂▃▄▂▃▂▃▃▂▁▁▂▂▃▁▂▃▂▂▃▂▂▂▂▃</td></tr><tr><td>train/recon_error_epoch</td><td>█▄▃▂▂▂▁▁</td></tr><tr><td>train/recon_error_step</td><td>▅▄█▄▃▃▃▃▃▂▄▃▂▃▃▂▂▂▃▂▂▂▃▃▂▂▁▁▂▁▂▁▂▁▁▂▂▂▂▃</td></tr><tr><td>train/visible_error_epoch</td><td>█▄▃▂▂▁▁▁</td></tr><tr><td>train/visible_error_step</td><td>██▇▇█▄▃▃▃▅▄▃▄▄▃▃▃▃▃▅▃▃▃▂▂▂▃▂▁▂▂▂▂▄▃▂▂▂▃▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇████</td></tr><tr><td>val/mae_loss</td><td>▆█▄▃▃▂▁▁</td></tr><tr><td>val/mask_ratio</td><td>▁▁▁▁▁▁▁▁</td></tr><tr><td>val/mask_ratio_viz</td><td>▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val/masked_error</td><td>▆█▄▃▃▂▁▁</td></tr><tr><td>val/recon_error</td><td>▇█▄▃▂▂▁▁</td></tr><tr><td>val/ssim_score</td><td>▃▁▅▆▆▇██</td></tr><tr><td>val/visible_error</td><td>█▇▅▂▂▁▁▂</td></tr><tr><td>val/volume_mask_ratio</td><td>▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>8</td></tr><tr><td>train/mae_loss_epoch</td><td>0.01113</td></tr><tr><td>train/mae_loss_step</td><td>0.01104</td></tr><tr><td>train/mask_ratio_epoch</td><td>0.75</td></tr><tr><td>train/mask_ratio_step</td><td>0.75</td></tr><tr><td>train/masked_error_epoch</td><td>0.02225</td></tr><tr><td>train/masked_error_step</td><td>0.02207</td></tr><tr><td>train/recon_error_epoch</td><td>0.02158</td></tr><tr><td>train/recon_error_step</td><td>0.02143</td></tr><tr><td>train/visible_error_epoch</td><td>0.01958</td></tr><tr><td>train/visible_error_step</td><td>0.01949</td></tr><tr><td>trainer/global_step</td><td>1549</td></tr><tr><td>val/mae_loss</td><td>0.0112</td></tr><tr><td>val/mask_ratio</td><td>0.75</td></tr><tr><td>val/mask_ratio_viz</td><td>0.75</td></tr><tr><td>val/masked_error</td><td>0.02241</td></tr><tr><td>val/recon_error</td><td>0.02175</td></tr><tr><td>val/ssim_score</td><td>0.97759</td></tr><tr><td>val/visible_error</td><td>0.01977</td></tr><tr><td>val/volume_mask_ratio</td><td>0.75</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">vesuvius_mae_vit3d_hybrid_size_256_patch_16_mask_0.75</strong> at: <a href='https://wandb.ai/joenader/vesuvius_mae_vit3d_hybrid/runs/9y3pwj3c' target=\"_blank\">https://wandb.ai/joenader/vesuvius_mae_vit3d_hybrid/runs/9y3pwj3c</a><br> View project at: <a href='https://wandb.ai/joenader/vesuvius_mae_vit3d_hybrid' target=\"_blank\">https://wandb.ai/joenader/vesuvius_mae_vit3d_hybrid</a><br>Synced 5 W&B file(s), 9 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250725_153955-9y3pwj3c/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import glob\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "import numpy as np\n",
    "import tifffile\n",
    "import wandb\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "import segmentation_models_pytorch as smp\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from volumentations import Compose, Rotate, RandomCropFromBorders, ElasticTransform, Resize, Flip, RandomRotate90, GaussianNoise, RandomGamma,RandomBrightnessContrast,GridDistortion\n",
    "from scipy import ndimage as ndi\n",
    "import matplotlib.pyplot as plt\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "# Import the modified MAE model\n",
    "# from modified_mae_vit3d import ViT3DMAE_WithViTDecoder, create_config_vesuvius\n",
    "\n",
    "\n",
    "class VesuviusFullVolumeDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for full 3D volumes from Vesuvius TIF files.\n",
    "    For MAE pretraining, we only need images (no labels required).\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset_folder, target_size=256, augment=False, mae_pretraining=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dataset_folder: Path to dataset folder containing imagesTr and labelsTr\n",
    "            target_size: Target size for volumes (will resize if needed)\n",
    "            augment: Whether to apply data augmentation\n",
    "            mae_pretraining: If True, only load images (no labels needed)\n",
    "        \"\"\"\n",
    "        self.dataset_folder = Path(dataset_folder)\n",
    "        self.target_size = target_size\n",
    "        self.augment = augment\n",
    "        self.mae_pretraining = mae_pretraining\n",
    "        \n",
    "        # Initialize augmentation pipeline\n",
    "        if self.augment:\n",
    "            self.aug_pipeline = self._get_augmentation(target_size)\n",
    "        \n",
    "        # Get image paths\n",
    "        self.images_dir = self.dataset_folder / \"imagesTr\"\n",
    "        \n",
    "        # Find all image files\n",
    "        self.samples = []\n",
    "        image_files = list(self.images_dir.glob(\"*.tif\"))\n",
    "        \n",
    "        print(f\"Found {len(image_files)} image files\")\n",
    "        \n",
    "        for img_path in image_files:\n",
    "            self.samples.append(img_path)\n",
    "        \n",
    "        print(f\"Total samples for MAE pretraining: {len(self.samples)}\")\n",
    "    \n",
    "    def _get_augmentation(self, patch_size):\n",
    "        \"\"\"Create volumentations augmentation pipeline\"\"\"\n",
    "        return Compose([\n",
    "            Rotate((-45, 45), (-45, 45), (-45, 45), p=0.1),\n",
    "            Flip(0, p=0.25),\n",
    "            Flip(1, p=0.25),\n",
    "            Flip(2, p=0.25),\n",
    "            RandomRotate90(p=0.25),\n",
    "            GaussianNoise(var_limit=(0, 5), p=0.2),\n",
    "            GridDistortion(num_steps=3, p=.1),\n",
    "        ], p=1.0)\n",
    "    \n",
    "    def _load_and_preprocess_volume(self, img_path):\n",
    "        \"\"\"Load and preprocess a single volume\"\"\"\n",
    "        # Load 3D volume\n",
    "        image_vol = tifffile.imread(str(img_path))\n",
    "        image_vol_dtype = image_vol.dtype\n",
    "        image_vol = image_vol.astype(np.float32)\n",
    "\n",
    "        # Resize if necessary\n",
    "        if image_vol.shape[0] != self.target_size:\n",
    "            image_vol = image_vol[:self.target_size, :self.target_size, :self.target_size]\n",
    "        \n",
    "        # Normalize image\n",
    "        if image_vol_dtype == np.uint16:\n",
    "            image_vol = image_vol / 65535\n",
    "        else:\n",
    "            image_vol = image_vol / 255\n",
    "        \n",
    "        return image_vol\n",
    "    \n",
    "    def _augment_volume(self, image_vol):\n",
    "        \"\"\"Apply 3D augmentations using volumentations\"\"\"\n",
    "        if not self.augment:\n",
    "            return image_vol\n",
    "        \n",
    "        # Convert to uint8 for volumentations (expects 0-255 range)\n",
    "        image_vol_uint8 = (image_vol * 255).astype(np.uint8)\n",
    "        \n",
    "        # Apply augmentation (only to image for MAE)\n",
    "        data = {'image': image_vol_uint8}\n",
    "        aug_data = self.aug_pipeline(**data)\n",
    "        \n",
    "        # Convert back to float32 and normalize\n",
    "        image_vol = aug_data['image'].astype(np.float32) / 255.0\n",
    "        \n",
    "        return image_vol\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.samples[idx]\n",
    "        \n",
    "        # Load and preprocess\n",
    "        image_vol = self._load_and_preprocess_volume(img_path)\n",
    "        \n",
    "        # Apply augmentation\n",
    "        image_vol = self._augment_volume(image_vol)\n",
    "        \n",
    "        # Convert to tensor\n",
    "        image_tensor = torch.FloatTensor(image_vol).unsqueeze(0)  # Add channel dimension\n",
    "        \n",
    "        # For MAE, we return the same image as both input and target\n",
    "        return image_tensor, image_tensor\n",
    "\n",
    "\n",
    "class VesuviusMAE_ViT3D_PLModel(pl.LightningModule):\n",
    "    \"\"\"MAE Pretraining Model using ViT3DSegmentation encoder and decoder\"\"\"\n",
    "    def __init__(self, input_size=256, patch_size=16, lr=1e-4, weight_decay=1e-4, mask_ratio=0.75):\n",
    "        super(VesuviusMAE_ViT3D_PLModel, self).__init__()\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        # Add storage for first validation sample\n",
    "        self.first_val_sample = None\n",
    "        \n",
    "        # Create MAE configuration matching ViT3DSegmentation\n",
    "        self.mae_config = create_config_vesuvius(\n",
    "            input_size=input_size,\n",
    "            patch_size=patch_size,\n",
    "            mask_ratio=mask_ratio\n",
    "        )\n",
    "        \n",
    "        # Create modified MAE model with ViT3DSegmentation components\n",
    "        self.model = ViT3DMAE_WithViTDecoder(self.mae_config)\n",
    "        \n",
    "        print(f\"Modified MAE Model initialized:\")\n",
    "        print(f\"  Input size: {input_size}x{input_size}x{input_size}\")\n",
    "        print(f\"  Patch size: {patch_size}x{patch_size}x{patch_size}\")\n",
    "        print(f\"  Mask ratio: {mask_ratio}\")\n",
    "        print(f\"  Encoder dim: {self.mae_config.dim}\")\n",
    "        print(f\"  Decoder dim: {self.mae_config.decoder_dim}\")\n",
    "        print(f\"  Using ViT3DSegmentation encoder and decoder components\")\n",
    "    \n",
    "    def forward(self, x, mask_ratio=None):\n",
    "        \"\"\"Forward pass through modified MAE\"\"\"\n",
    "        if mask_ratio is None:\n",
    "            mask_ratio = self.mae_config.mask_ratio\n",
    "        \n",
    "        loss, pred, mask = self.model(x, mask_ratio=mask_ratio)\n",
    "        \n",
    "        return loss, pred, mask\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, _ = batch  # Ignore labels for MAE pretraining\n",
    "        \n",
    "        # MAE forward pass\n",
    "        \n",
    "        loss, pred, mask = self(x)\n",
    "        # print(pred.max().item(),pred.min().item(),x.max().item(),x.min().item())\n",
    "        # Calculate reconstruction metrics\n",
    "        with torch.no_grad():\n",
    "            # Calculate reconstruction quality metrics\n",
    "            pred_error = ((pred - x) ** 2).mean()\n",
    "            \n",
    "            # Mask statistics\n",
    "            mask_ratio_actual = mask.float().mean()\n",
    "            \n",
    "            # Volume-level metrics\n",
    "            volume_mask = self.model.patch_mask_to_volume_mask(mask)\n",
    "            masked_region_error = ((pred - x) ** 2 * volume_mask).sum() / (volume_mask.sum() + 1e-8)\n",
    "            visible_region_error = ((pred - x) ** 2 * (1 - volume_mask)).sum() / ((1 - volume_mask).sum() + 1e-8)\n",
    "        \n",
    "        # Log metrics\n",
    "        self.log(\"train/mae_loss\", loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"train/recon_error\", pred_error, on_step=True, on_epoch=True)\n",
    "        self.log(\"train/masked_error\", masked_region_error, on_step=True, on_epoch=True)\n",
    "        self.log(\"train/visible_error\", visible_region_error, on_step=True, on_epoch=True)\n",
    "        self.log(\"train/mask_ratio\", mask_ratio_actual, on_step=True, on_epoch=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, _ = batch  # Ignore labels for MAE pretraining\n",
    "        \n",
    "        # MAE forward pass\n",
    "        loss, pred, mask = self(x)\n",
    "        \n",
    "        # Calculate reconstruction metrics\n",
    "        with torch.no_grad():\n",
    "            pred_error = ((pred - x) ** 2).mean()\n",
    "            mask_ratio_actual = mask.float().mean()\n",
    "            \n",
    "            # Volume-level metrics\n",
    "            volume_mask = self.model.patch_mask_to_volume_mask(mask)\n",
    "            masked_region_error = ((pred - x) ** 2 * volume_mask).sum() / (volume_mask.sum() + 1e-8)\n",
    "            visible_region_error = ((pred - x) ** 2 * (1 - volume_mask)).sum() / ((1 - volume_mask).sum() + 1e-8)\n",
    "            \n",
    "            # Reconstruction quality score (higher is better)\n",
    "            ssim_score = 1 - masked_region_error.item()  # Simplified SSIM-like metric\n",
    "        \n",
    "        # Log metrics\n",
    "        self.log(\"val/mae_loss\", loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"val/recon_error\", pred_error, on_step=False, on_epoch=True)\n",
    "        self.log(\"val/masked_error\", masked_region_error, on_step=False, on_epoch=True)\n",
    "        self.log(\"val/visible_error\", visible_region_error, on_step=False, on_epoch=True)\n",
    "        self.log(\"val/mask_ratio\", mask_ratio_actual, on_step=False, on_epoch=True)\n",
    "        self.log(\"val/ssim_score\", ssim_score, on_step=False, on_epoch=True)\n",
    "        \n",
    "        # Store first validation sample for visualization\n",
    "        if batch_idx == 0 and self.first_val_sample is None:\n",
    "            self.first_val_sample = {\n",
    "                'original': x[0:1].cpu(),\n",
    "                'reconstructed': pred[0:1].cpu(),\n",
    "                'mask': mask[0:1].cpu(),\n",
    "                'volume_mask': volume_mask[0:1].cpu()\n",
    "            }\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def visualize_mae_reconstruction_3d(self, original, reconstructed, volume_mask, num_slices=3):\n",
    "        \"\"\"Visualize MAE reconstruction results in 3D slices\"\"\"\n",
    "        # Remove batch and channel dimensions\n",
    "        original = original.squeeze().numpy()\n",
    "        reconstructed = reconstructed.squeeze().numpy()\n",
    "        volume_mask = volume_mask.squeeze().numpy()\n",
    "        \n",
    "        d, h, w = original.shape\n",
    "        slice_indices = np.linspace(0, d-1, num_slices, dtype=int)\n",
    "        \n",
    "        fig, axes = plt.subplots(4, num_slices, figsize=(15, 10))\n",
    "        if num_slices == 1:\n",
    "            axes = axes.reshape(-1, 1)\n",
    "        \n",
    "        for i, slice_idx in enumerate(slice_indices):\n",
    "            # Original\n",
    "            axes[0, i].imshow(original[slice_idx], cmap='gray', vmin=0, vmax=1)\n",
    "            axes[0, i].set_title(f'Original Z={slice_idx}')\n",
    "            axes[0, i].axis('off')\n",
    "            \n",
    "            # Reconstructed\n",
    "            axes[1, i].imshow(reconstructed[slice_idx], cmap='gray', vmin=0, vmax=1)\n",
    "            axes[1, i].set_title(f'Reconstructed Z={slice_idx}')\n",
    "            axes[1, i].axis('off')\n",
    "            \n",
    "            # Volume mask (masked regions in red)\n",
    "            axes[2, i].imshow(volume_mask[slice_idx], cmap='gray', vmin=0, vmax=1)\n",
    "            axes[2, i].set_title(f'Mask Z={slice_idx}')\n",
    "            axes[2, i].axis('off')\n",
    "            \n",
    "            # Difference (error visualization)\n",
    "            diff = np.abs(original[slice_idx] - reconstructed[slice_idx])\n",
    "            axes[3, i].imshow(diff, cmap='hot', vmin=0, vmax=0.5)\n",
    "            axes[3, i].set_title(f'Abs Difference Z={slice_idx}')\n",
    "            axes[3, i].axis('off')\n",
    "        \n",
    "        plt.suptitle('MAE Reconstruction with ViT3D Components', fontsize=14)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        with tempfile.NamedTemporaryFile(suffix='.png', delete=False) as tmp:\n",
    "            plt.savefig(tmp.name, dpi=100, bbox_inches='tight')\n",
    "            tmp_path = tmp.name\n",
    "        plt.close()\n",
    "        \n",
    "        return tmp_path\n",
    "    \n",
    "    def on_validation_epoch_end(self):\n",
    "        if self.first_val_sample is not None:\n",
    "            original = self.first_val_sample['original']\n",
    "            reconstructed = self.first_val_sample['reconstructed']\n",
    "            mask = self.first_val_sample['mask']\n",
    "            volume_mask = self.first_val_sample['volume_mask']\n",
    "            \n",
    "            viz_path = self.visualize_mae_reconstruction_3d(original, reconstructed, volume_mask)\n",
    "            \n",
    "            if self.logger and hasattr(self.logger, 'experiment'):\n",
    "                self.logger.experiment.log({\n",
    "                    \"val/reconstruction_viz\": wandb.Image(viz_path),\n",
    "                    \"val/mask_ratio_viz\": mask.float().mean().item(),\n",
    "                    \"val/volume_mask_ratio\": volume_mask.float().mean().item()\n",
    "                })\n",
    "            \n",
    "            os.unlink(viz_path)\n",
    "            \n",
    "        self.first_val_sample = None\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        # Use AdamW with cosine annealing for MAE pretraining\n",
    "        optimizer = AdamW(self.parameters(), lr=self.hparams.lr, weight_decay=self.hparams.weight_decay)\n",
    "        \n",
    "        def lr_lambda(epoch):\n",
    "            warmup_epochs = 1\n",
    "            if epoch < warmup_epochs:\n",
    "                return epoch / warmup_epochs\n",
    "            return 0.5 * (1 + np.cos(np.pi * (epoch - warmup_epochs) / (self.trainer.max_epochs - warmup_epochs)))\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10, eta_min=1e-6)        \n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "\n",
    "class CFG:\n",
    "    \"\"\"Configuration class for modified MAE pretraining\"\"\"\n",
    "    # Dataset\n",
    "    dataset_folder = './Dataset091'  # Change this to your dataset path\n",
    "    \n",
    "    # Model\n",
    "    input_size = 256      # Size of input volumes\n",
    "    patch_size = 16       # Patch size for ViT\n",
    "    mask_ratio = 0.75     # Masking ratio for MAE\n",
    "    \n",
    "    # Training\n",
    "    batch_size = 8        # Small batch size due to large volumes\n",
    "    num_workers = 32      # Reduced workers for large volumes\n",
    "    lr = 3e-4             # Learning rate for MAE\n",
    "    weight_decay = 0.00005   # Weight decay for MAE\n",
    "    epochs = 500          # More epochs for pretraining\n",
    "    \n",
    "    # Data split\n",
    "    train_ratio = 0.99    # Use more data for pretraining\n",
    "    \n",
    "    # Experiment\n",
    "    exp_name = 'vesuvius_mae_vit3d_hybrid'\n",
    "    \n",
    "    # Paths\n",
    "    model_dir = './outputs/vesuvius_mae_vit3d/'\n",
    "    \n",
    "    def __init__(self):\n",
    "        os.makedirs(self.model_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "def split_dataset(dataset, train_ratio=0.8):\n",
    "    \"\"\"Split dataset into train and validation\"\"\"\n",
    "    num_samples = len(dataset)\n",
    "    num_train = int(num_samples * train_ratio)\n",
    "    \n",
    "    indices = torch.randperm(num_samples)\n",
    "    train_indices = indices[:num_train]\n",
    "    val_indices = indices[num_train:]\n",
    "    \n",
    "    train_dataset = torch.utils.data.Subset(dataset, train_indices)\n",
    "    val_dataset = torch.utils.data.Subset(dataset, val_indices)\n",
    "    \n",
    "    return train_dataset, val_dataset\n",
    "\n",
    "\n",
    "def count_dataset_stats(dataset_folder):\n",
    "    \"\"\"Count basic statistics about the dataset\"\"\"\n",
    "    dataset_folder = Path(dataset_folder)\n",
    "    images_dir = dataset_folder / \"imagesTr\"\n",
    "    \n",
    "    image_files = list(images_dir.glob(\"*.tif\"))\n",
    "    print(f\"Found {len(image_files)} image files\")\n",
    "    \n",
    "    # Sample a few files to check shapes and statistics\n",
    "    for i, img_path in enumerate(image_files[:3]):\n",
    "        try:\n",
    "            image_vol = tifffile.imread(str(img_path))\n",
    "            print(f\"  {img_path.stem}:\")\n",
    "            print(f\"    Image shape: {image_vol.shape}\")\n",
    "            print(f\"    Image range: [{image_vol.min()}, {image_vol.max()}]\")\n",
    "        except Exception as e:\n",
    "            print(f\"    Error reading {img_path}: {e}\")\n",
    "\n",
    "\n",
    "def test_model_compatibility():\n",
    "    \"\"\"Test that the modified MAE model works correctly\"\"\"\n",
    "    print(\"Testing modified MAE model...\")\n",
    "    \n",
    "    # Create config and model\n",
    "    config = create_config_vesuvius(input_size=256, patch_size=16, mask_ratio=0.75)\n",
    "    model = ViT3DMAE_WithViTDecoder(config)\n",
    "    \n",
    "    # Test with dummy data\n",
    "    batch_size = 1\n",
    "    dummy_input = torch.randn(batch_size, 1, 256, 256, 256)\n",
    "    \n",
    "    print(f\"Testing with input shape: {dummy_input.shape}\")\n",
    "    \n",
    "    # Test forward pass\n",
    "    with torch.no_grad():\n",
    "        loss, pred, mask = model(dummy_input)\n",
    "        print(f\"✓ Forward pass successful\")\n",
    "        print(f\"  Loss: {loss.item():.4f}\")\n",
    "        print(f\"  Output shape: {pred.shape}\")\n",
    "        print(f\"  Mask ratio: {mask.float().mean().item():.3f}\")\n",
    "        \n",
    "        # Test encoder only\n",
    "        encoded, mask_enc, ids_restore = model.forward_encoder_only(dummy_input, mask_ratio=0.75)\n",
    "        print(f\"✓ Encoder-only pass successful\")\n",
    "        print(f\"  Encoded shape: {encoded.shape}\")\n",
    "        \n",
    "        # Test patch embeddings\n",
    "        patch_emb = model.get_patch_embeddings(dummy_input)\n",
    "        print(f\"✓ Patch embeddings successful\")\n",
    "        print(f\"  Patch embeddings shape: {patch_emb.shape}\")\n",
    "    \n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"✓ Total parameters: {total_params:,}\")\n",
    "    print(\"Model compatibility test passed!\")\n",
    "    return True\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main MAE pretraining function with ViT3D components\"\"\"\n",
    "    cfg = CFG()\n",
    "    \n",
    "    # Count dataset statistics\n",
    "    print(\"\\nDataset statistics:\")\n",
    "    count_dataset_stats(cfg.dataset_folder)\n",
    "    \n",
    "    # Create full dataset for MAE pretraining\n",
    "    print(f\"\\nCreating MAE dataset with target size {cfg.input_size}...\")\n",
    "    full_dataset = VesuviusFullVolumeDataset(\n",
    "        dataset_folder=cfg.dataset_folder,\n",
    "        target_size=cfg.input_size,\n",
    "        augment=True,\n",
    "        mae_pretraining=True  # Only load images, no labels\n",
    "    )\n",
    "    \n",
    "    # Split into train and validation\n",
    "    train_dataset, val_dataset = split_dataset(full_dataset, cfg.train_ratio)\n",
    "    print(f\"Training samples: {len(train_dataset)}\")\n",
    "    print(f\"Validation samples: {len(val_dataset)}\")\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=cfg.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=cfg.num_workers,\n",
    "        pin_memory=True,\n",
    "        drop_last=True\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=cfg.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=cfg.num_workers,\n",
    "        pin_memory=True,\n",
    "        drop_last=False\n",
    "    ) if len(val_dataset) > 0 else None\n",
    "    \n",
    "    # Create modified MAE model\n",
    "    model = VesuviusMAE_ViT3D_PLModel(\n",
    "        input_size=cfg.input_size,\n",
    "        patch_size=cfg.patch_size,\n",
    "        lr=cfg.lr,\n",
    "        weight_decay=cfg.weight_decay,\n",
    "        mask_ratio=cfg.mask_ratio\n",
    "    )\n",
    "    \n",
    "    # Print model info\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"\\nModified MAE Model parameters: {total_params:,}\")\n",
    "    \n",
    "    # Memory estimate\n",
    "    batch_memory_gb = (cfg.batch_size * 1 * cfg.input_size**3 * 4) / (1024**3)\n",
    "    print(f\"Estimated memory per batch: {batch_memory_gb:.2f} GB\")\n",
    "    \n",
    "    # Setup logging\n",
    "    wandb_logger = WandbLogger(\n",
    "        project=\"vesuvius_mae_vit3d_hybrid\",\n",
    "        name=f\"{cfg.exp_name}_size_{cfg.input_size}_patch_{cfg.patch_size}_mask_{cfg.mask_ratio}\"\n",
    "    )\n",
    "    \n",
    "    # Setup callbacks\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        dirpath=cfg.model_dir,\n",
    "        filename=f'vesuvius_mae_vit3d_{{epoch:02d}}_{{val_mae_loss:.4f}}',\n",
    "        monitor='val/mae_loss' if val_loader else 'train/mae_loss',\n",
    "        mode='min',\n",
    "        save_top_k=5,\n",
    "        save_last=True\n",
    "    )\n",
    "    \n",
    "    # Create trainer\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=cfg.epochs,\n",
    "        accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n",
    "        devices=1,\n",
    "        logger=wandb_logger,\n",
    "        callbacks=[checkpoint_callback],\n",
    "        precision='16-mixed' if torch.cuda.is_available() else 32,\n",
    "        gradient_clip_val=1.0,\n",
    "        log_every_n_steps=10,\n",
    "        accumulate_grad_batches=32,  # Gradient accumulation\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    print(f\"\\nStarting modified MAE pretraining for {cfg.epochs} epochs...\")\n",
    "    print(f\"Mask ratio: {cfg.mask_ratio}\")\n",
    "    print(f\"Using ViT3DSegmentation encoder and decoder components\")\n",
    "    print(f\"Effective batch size: {cfg.batch_size * 8}\")  # Due to gradient accumulation\n",
    "    \n",
    "    trainer.fit(model, train_loader, val_loader)\n",
    "    \n",
    "    print(\"Modified MAE pretraining completed!\")\n",
    "    print(f\"Saved model checkpoints in: {cfg.model_dir}\")\n",
    "    wandb.finish()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58beb5ea-5892-4713-a655-e24a1b552f79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
